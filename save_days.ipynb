{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = glob.glob('data/full_files/*.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping ambiguous time for date: 2006-02-18\n",
      "Skipping ambiguous time for date: 2006-11-05\n",
      "Skipping ambiguous time for date: 2007-02-24\n",
      "Skipping ambiguous time for date: 2007-10-14\n",
      "Skipping ambiguous time for date: 2008-02-16\n",
      "Skipping ambiguous time for date: 2008-10-19\n",
      "Skipping ambiguous time for date: 2009-02-14\n",
      "Skipping ambiguous time for date: 2009-10-18\n",
      "Skipping ambiguous time for date: 2010-02-20\n",
      "Skipping ambiguous time for date: 2010-10-17\n",
      "Skipping ambiguous time for date: 2011-02-19\n",
      "Skipping ambiguous time for date: 2011-10-16\n",
      "Skipping ambiguous time for date: 2012-02-25\n",
      "Skipping ambiguous time for date: 2012-10-21\n",
      "Skipping ambiguous time for date: 2013-02-16\n",
      "Skipping ambiguous time for date: 2013-10-20\n",
      "Skipping ambiguous time for date: 2014-02-15\n",
      "Skipping ambiguous time for date: 2014-10-19\n",
      "Skipping ambiguous time for date: 2015-02-21\n",
      "Skipping ambiguous time for date: 2015-10-18\n",
      "Skipping ambiguous time for date: 2016-02-20\n",
      "Skipping ambiguous time for date: 2016-10-16\n",
      "Skipping ambiguous time for date: 2017-02-18\n",
      "Skipping ambiguous time for date: 2017-10-15\n",
      "Skipping ambiguous time for date: 2018-02-17\n",
      "Skipping ambiguous time for date: 2018-11-04\n",
      "Skipping ambiguous time for date: 2019-02-16\n",
      "Skipping ambiguous time for date: 2005-02-19\n",
      "Skipping ambiguous time for date: 2005-10-16\n",
      "Skipping ambiguous time for date: 2006-02-18\n",
      "Skipping ambiguous time for date: 2006-11-05\n",
      "Skipping ambiguous time for date: 2007-02-24\n",
      "Skipping ambiguous time for date: 2007-10-14\n",
      "Skipping ambiguous time for date: 2008-02-16\n",
      "Skipping ambiguous time for date: 2008-10-19\n",
      "Skipping ambiguous time for date: 2009-02-14\n",
      "Skipping ambiguous time for date: 2009-10-18\n",
      "Skipping ambiguous time for date: 2010-02-20\n",
      "Skipping ambiguous time for date: 2010-10-17\n",
      "Skipping ambiguous time for date: 2011-02-19\n",
      "Skipping ambiguous time for date: 2011-10-16\n",
      "Skipping ambiguous time for date: 2012-02-25\n",
      "Skipping ambiguous time for date: 2012-10-21\n",
      "Skipping ambiguous time for date: 2013-02-16\n",
      "Skipping ambiguous time for date: 2013-10-20\n",
      "Skipping ambiguous time for date: 2014-02-15\n",
      "Skipping ambiguous time for date: 2014-10-19\n",
      "Skipping ambiguous time for date: 2015-02-21\n",
      "Skipping ambiguous time for date: 2015-10-18\n",
      "Skipping ambiguous time for date: 2016-02-20\n",
      "Skipping ambiguous time for date: 2016-10-16\n",
      "Skipping ambiguous time for date: 2017-02-18\n",
      "Skipping ambiguous time for date: 2017-10-15\n",
      "Skipping ambiguous time for date: 2018-02-17\n",
      "Skipping ambiguous time for date: 2018-11-04\n",
      "Skipping ambiguous time for date: 2019-02-16\n",
      "Skipping ambiguous time for date: 2006-02-18\n",
      "Skipping ambiguous time for date: 2006-11-05\n",
      "Skipping ambiguous time for date: 2007-02-24\n",
      "Skipping ambiguous time for date: 2007-10-14\n",
      "Skipping ambiguous time for date: 2008-02-16\n",
      "Skipping ambiguous time for date: 2008-10-19\n",
      "Skipping ambiguous time for date: 2009-02-14\n",
      "Skipping ambiguous time for date: 2009-10-18\n",
      "Skipping ambiguous time for date: 2010-02-20\n",
      "Skipping ambiguous time for date: 2010-10-17\n",
      "Skipping ambiguous time for date: 2011-02-19\n",
      "Skipping ambiguous time for date: 2011-10-16\n",
      "Skipping ambiguous time for date: 2012-02-25\n",
      "Skipping ambiguous time for date: 2012-10-21\n",
      "Skipping ambiguous time for date: 2013-02-16\n",
      "Skipping ambiguous time for date: 2013-10-20\n",
      "Skipping ambiguous time for date: 2014-02-15\n",
      "Skipping ambiguous time for date: 2014-10-19\n",
      "Skipping ambiguous time for date: 2015-02-21\n",
      "Skipping ambiguous time for date: 2015-10-18\n",
      "Skipping ambiguous time for date: 2016-02-20\n",
      "Skipping ambiguous time for date: 2016-10-16\n",
      "Skipping ambiguous time for date: 2017-02-18\n",
      "Skipping ambiguous time for date: 2017-10-15\n",
      "Skipping ambiguous time for date: 2018-02-17\n",
      "Skipping ambiguous time for date: 2018-11-04\n",
      "Skipping ambiguous time for date: 2019-02-16\n",
      "Skipping ambiguous time for date: 2015-03-29\n",
      "Skipping ambiguous time for date: 2015-10-25\n",
      "Skipping ambiguous time for date: 2016-03-27\n",
      "Skipping ambiguous time for date: 2016-10-30\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import os\n",
    "import pytz\n",
    "from timezonefinder import TimezoneFinder\n",
    "\n",
    "def utc_to_local(dataset, latitude, longitude):\n",
    "    tf = TimezoneFinder()\n",
    "    timezone_str = tf.certain_timezone_at(lat=latitude, lng=longitude)\n",
    "    if timezone_str is None:\n",
    "        print(f\"Warning: Could not determine the time zone for latitude={latitude}, longitude={longitude}. Using UTC.\")\n",
    "        return dataset\n",
    "    else:\n",
    "        timezone = pytz.timezone(timezone_str)\n",
    "        dataset['time'] = pd.to_datetime(dataset['time'].values).tz_localize(pytz.UTC).tz_convert(timezone)\n",
    "        return dataset\n",
    "\n",
    "for station in stations:\n",
    "    large_ds = xr.open_dataset(station)\n",
    "    station_name = station.split('/')[-1].split('.')[0]\n",
    "    \n",
    "    latitude = float(large_ds.attrs.get('geospatial_lat_min'))\n",
    "    longitude = float(large_ds.attrs.get('geospatial_lon_min'))\n",
    "    elevation = large_ds.attrs.get('geospatial_vertical_min')\n",
    "    \n",
    "    # Convert time from UTC to local time\n",
    "    large_ds = utc_to_local(large_ds, latitude, longitude)\n",
    "    \n",
    "    dates = large_ds.indexes['time'].to_series().dt.date.unique()\n",
    "\n",
    "    # Filter dates to include only those after January 1, 2005\n",
    "    dates = dates[dates >= pd.to_datetime('2005-01-01').date()]\n",
    "\n",
    "    # Iterate over each unique date\n",
    "    for date in dates:\n",
    "        output_file = f\"data/daily/{station_name}_{date.strftime('%Y%m%d')}.nc\"\n",
    "        \n",
    "        if not os.path.exists(output_file):\n",
    "            try:\n",
    "                # Select data for the current date\n",
    "                mini_ds = large_ds.sel(time=str(date))\n",
    "                \n",
    "                # Save the mini Dataset to a new NetCDF file\n",
    "                mini_ds.to_netcdf(output_file)\n",
    "            except:\n",
    "                print(f\"Skipping ambiguous time for date: {date}\")\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159375"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "files = glob.glob('data/daily/*nc')\n",
    "\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Directory containing the xarray files\n",
    "directory = \"data/daily\"\n",
    "\n",
    "# Directory to move the files that meet the condition\n",
    "output_directory = \"data/daily_processed\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# List of variables to check for NaN percentage\n",
    "variables = [\"GHI\", \"DHI\", \"BNI\", \"ghi_extra\", \"ghi_clear\", \"bhi_clear\", \"dhi_clear\", \"dni_clear\", \"ghi\", \"bhi\", \"dhi\", \"dni\"]\n",
    "\n",
    "# Threshold for non-NaN percentage\n",
    "threshold = 0.8\n",
    "\n",
    "# Iterate over the files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".nc\"):  # Assuming the files have a \".nc\" extension\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        # Open the xarray dataset\n",
    "        ds = xr.open_dataset(file_path)\n",
    "        \n",
    "        # Check the percentage of non-NaN values for each variable\n",
    "        valid_vars = []\n",
    "        for var in variables:\n",
    "            if var in ds:\n",
    "                non_nan_percentage = ds[var].count() / len(ds[var])\n",
    "                if non_nan_percentage >= threshold:\n",
    "                    valid_vars.append(var)\n",
    "        \n",
    "        # If all specified variables have more than 80% non-NaN values\n",
    "        if len(valid_vars) == len(variables):\n",
    "            # Interpolate NaN values for all variables\n",
    "            ds = ds.interpolate_na(dim=\"time\")\n",
    "            \n",
    "            # Save the modified dataset to the output directory\n",
    "            output_path = os.path.join(output_directory, filename)\n",
    "            ds.to_netcdf(output_path)\n",
    "            \n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101375"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "files = glob.glob('data/daily_processed/*nc')\n",
    "\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
